networks:
  data-network:
    name: data-network
    driver: bridge
  

services:
# Kafka broker 
  kafka:
    image: bitnami/kafka:3.9
    container_name: kafka
    ports:
      - 32967:32967
      - 9092:9092
    volumes:
      - ./kafka:/bitnami/kafka
    environment:
      # KRaft settings
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_BROKER_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller, broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:2181
      # Listeners
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:2181,PLAINTEXT_HOST://:32967
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092,PLAINTEXT_HOST://:32967
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_KRAFT_CLUSTER_ID=MkU3OEVBNTcwNTJENDM2Qk
    networks:
      - data-network

# Kafka UI
  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - '8080:8080'
    depends_on:
      - kafka
    environment:
      DYNAMIC_CONFIG_ENABLED: true
    volumes:
      - ./kafka-ui/config.yml:/etc/kafkaui/dynamic_config.yaml
    networks:
      - data-network

# Flink taskmanager 1
  taskmanager-1:
    image: pyflink:1.18.1
    container_name: taskmanager-1
    volumes:
      # Connectors in opt/flink/opt
      - ./connectors/flink-sql-connector-kafka-3.0.2-1.18.jar:/opt/flink/opt/flink-sql-connector-kafka-3.0.2-1.18.jar
      - ./connectors/flink-connector-cassandra_2.12-3.2.0-1.18.jar:/opt/flink/lib/flink-connector-cassandra_2.12-3.2.0-1.18.jar
      
      # User's pyflink jobs
      - ./usrlib/screen_2_q6_q8_flink_job.py:/opt/flink/usrlib/screen_2_q6_q8_flink_job.py
      - ./usrlib/screen_3_q9_q10_flink_job.py:/opt/flink/usrlib/screen_3_q9_q10_flink_job.py
      - ./usrlib/screen_4_q11_q15_flink_job.py:/opt/flink/usrlib/screen_4_q11_q15_flink_job.py
      
      # Screen 2 screen accelerated
      - ./usrlib/screen_2_q6_q8_flink_job_one_datastream.py:/opt/flink/usrlib/screen_2_q6_q8_flink_job_one_datastream.py
      - ./usrlib/screen_2_q6_q8_flink_job_testing.py:/opt/flink/usrlib/screen_2_q6_q8_flink_job_testing.py
      - ./usrlib/screen_2_q6_q8_flink_job_testing_concurrent.py:/opt/flink/usrlib/screen_2_q6_q8_flink_job_testing_concurrent.py

      # Config file
      - ./usrlib/getting-started-in-docker.ini:/opt/flink/usrlib/getting-started-in-docker.ini
    depends_on:
      - jobmanager
    command: taskmanager
    # scale: 3
    environment:
      - | 
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 32
        parallelism.default: 4
        rest.flamegraph.enabled: true
        metrics.latency.interval: 1000
        
    # python.operator-chaining.enabled: false
    # taskmanager.rpc.port: 36329
    # taskmanager.data.port: 36330

    # Increase resources for the taskmanager-1
    deploy:
      resources:
        limits:
          cpus: '4.0'   # Limit this container to 4 CPUs
          memory: 8g
        reservations:
          cpus: '2.0'   # Guarantee at least 2 CPUs
          memory: 4g


    # ports: 
    #   - 36329:36329
    #   - 36330:36330
    networks:
      - data-network

# Flink jobmanager
  jobmanager:
    image: pyflink:1.18.1
    container_name: jobmanager
    volumes:
    # Screen 1  
      - ./usrlib/near-real-time-stats-and-popularity-insights-via-flink.py:/opt/flink/usrlib/near-real-time-stats-and-popularity-insights-via-flink.py
      - ./usrlib/create_raw_events_per_sec_datastream.py:/opt/flink/usrlib/create_raw_events_per_sec_datastream.py
      - ./usrlib/near-real-time-stars-forks-via-flink.py:/opt/flink/usrlib/near-real-time-stars-forks-via-flink.py
      
    # Connectors
      - ./connectors/flink-sql-connector-kafka-3.0.2-1.18.jar:/opt/flink/opt/flink-sql-connector-kafka-3.0.2-1.18.jar
      - ./connectors/flink-connector-cassandra_2.12-3.2.0-1.18.jar:/opt/flink/opt/flink-connector-cassandra_2.12-3.2.0-1.18.jar

    # User's pyflink jobs
      - ./usrlib/screen_2_q6_q8_flink_job.py:/opt/flink/usrlib/screen_2_q6_q8_flink_job.py
      - ./usrlib/screen_3_q9_q10_flink_job.py:/opt/flink/usrlib/screen_3_q9_q10_flink_job.py
      - ./usrlib/screen_4_q11_q15_flink_job.py:/opt/flink/usrlib/screen_4_q11_q15_flink_job.py
    
    # Screen 2 screen accelerated
      - ./usrlib/screen_2_q6_q8_flink_job_one_datastream.py:/opt/flink/usrlib/screen_2_q6_q8_flink_job_one_datastream.py
      - ./usrlib/screen_2_q6_q8_flink_job_testing.py:/opt/flink/usrlib/screen_2_q6_q8_flink_job_testing.py
      - ./usrlib/screen_2_q6_q8_flink_job_testing_concurrent.py:/opt/flink/usrlib/screen_2_q6_q8_flink_job_testing_concurrent.py


    # Config file
      - ./usrlib/getting-started-in-docker.ini:/opt/flink/usrlib/getting-started-in-docker.ini
    ports:
      - "8081:8081"
    command: jobmanager    
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 32
        parallelism.default: 4
        rest.flamegraph.enabled: true
        metrics.latency.interval: 1000

      # python.operator-chaining.enabled: false
    depends_on: 
      - cassandra_stelios
    networks:
      - data-network

# Cassandra 
  cassandra_stelios:
    image: cassandra:4.1.7
    ports:
      - "9142:9142"
    container_name: cassandra_stelios
    hostname: cassandra_stelios
    volumes:
      - /var/lib/cassandra:/var/lib/cassandra
      - ./cassandra.yaml:/etc/cassandra/cassandra.yaml
    networks:
      - data-network
    healthcheck:
      test: ["CMD-SHELL", "cqlsh cassandra_stelios 9142 -e 'describe keyspaces'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s

# Cassandra UI
  cassandra-ui:
    image: ipushc/cassandra-web
    container_name: cassandra-ui
    ports:
      - "8083:8083"
    depends_on:
      cassandra_stelios:
        condition: service_healthy
    networks:
      - data-network
    environment: 
      - CASSANDRA_HOST=cassandra_stelios
      - CASSANDRA_PORT=9142

# Python server exposing data
  python-data-exposing-server:
    image: python:3.10-script-executing-image
    container_name: python-data-exposing-server
    volumes: 
      - ./flask-material-dashboard/server.py:/usrlib/server.py
    ports: 
      # - "127.0.0.1:3100:3100"
      - "3100:3100"
    command: ['python', 'server.py']
    
    environment:
      - PYTHONUNBUFFERED=1
    depends_on:
      - cassandra_stelios
    networks:
      - data-network

# Python flask UI
  python-flask-app:
    image: python:3.10-script-executing-image
    container_name: python-flask-app
    volumes: 
      - ./flask-material-dashboard:/usrlib/flask-material-dashboard
      - ./usrlib/getting-started-in-docker.ini:/helpers/getting-started-in-docker.ini
    ports: 
      - "5000:5000"
    
    command: ['python', 'flask-material-dashboard/run_with_socketio.py', '/helpers/getting-started-in-docker.ini']
    
    environment:
      - PYTHONUNBUFFERED=1
    depends_on:
      - kafka
    networks:
      - data-network


# # 1. Historical events ingestion
# Historical events producer
  python-historical-events-producer:
    image: python:3.10-script-executing-image
    container_name: python-historical-events-producer
    volumes: 
      # /usrlib
      - ./usrlib/historical-events-file-rotating-producer.py:/usrlib/historical-events-file-rotating-producer.py
      - ./usrlib/produce_from_last_line_of_file.py:/usrlib/produce_from_last_line_of_file.py
      - ./usrlib/get_parsed_gharchive_files.py:/usrlib/get_parsed_gharchive_files.py
      - ./usrlib/thin_data.py:/usrlib/thin_data.py      
      - ./usrlib/check_job_status_multiple_jobs.py:/usrlib/check_job_status_multiple_jobs.py
      - ./usrlib/create_topic_from_inside_the_container.py:/usrlib/create_topic_from_inside_the_container.py
      
      # usrlib/helpers
      - ./usrlib/getting-started-in-docker.ini:/usrlib/helpers/getting-started-in-docker.ini

      # github_data
      - ./usrlib/github_data:/github_data
      - ./usrlib/github_data/files_parsed.json:/github_data/files_parsed.json
      
    command: ['python', 'historical-events-file-rotating-producer.py', '/usrlib/helpers/getting-started-in-docker.ini']
    

    environment:
      - PYTHONUNBUFFERED=1
    depends_on:
      - kafka
    networks:
      - data-network


# Producer of a small sample of the events of a file (Used for ingestion speed testing)
  python-100.000-events-producer:
    image: python:3.10-script-executing-image
    container_name: python-100.000-events-producer
    volumes: 
      # /usrlib
      - ./usrlib/100.000-events-producer.py:/usrlib/100.000-events-producer.py
      - ./usrlib/produce_from_last_line_of_file.py:/usrlib/produce_from_last_line_of_file.py
      - ./usrlib/get_parsed_gharchive_files.py:/usrlib/get_parsed_gharchive_files.py
      - ./usrlib/thin_data.py:/usrlib/thin_data.py      
      - ./usrlib/check_job_status_multiple_jobs.py:/usrlib/check_job_status_multiple_jobs.py
      - ./usrlib/create_topic_from_inside_the_container.py:/usrlib/create_topic_from_inside_the_container.py
      
      # usrlib/helpers
      - ./usrlib/getting-started-in-docker.ini:/usrlib/helpers/getting-started-in-docker.ini

      # github_data
      - ./usrlib/github_data_for_speed_testing:/github_data_for_speed_testing
      - ./usrlib/github_data_for_speed_testing/files_parsed.json:/github_data_for_speed_testing/files_parsed.json


      
    command: ['python', '100.000-events-producer.py', '/usrlib/helpers/getting-started-in-docker.ini']

    environment:
      - PYTHONUNBUFFERED=1
    depends_on:
      - kafka
    networks:
      - data-network



# # 2. Near real time events ingestion

# Near real time events producer
  python-near-real-time-events-producer:
    image: python:3.10-script-executing-image
    container_name: python-near-real-time-events-producer
    volumes: 
      - ./usrlib/near-real-time-producer.py:/usrlib/near-real-time-producer.py
      - ./usrlib/thin_data.py:/usrlib/thin_data.py
      - ./usrlib/produce_from_last_line_of_file.py:/usrlib/produce_from_last_line_of_file.py
      - ./usrlib/check_job_status_multiple_jobs.py:/usrlib/check_job_status_multiple_jobs.py

      - ./usrlib/getting-started-in-docker.ini:/usrlib/helpers/getting-started-in-docker.ini
      - ./usrlib/get_parsed_gharchive_files.py:/usrlib/get_parsed_gharchive_files.py
      - ./usrlib/create_topic_from_inside_the_container.py:/usrlib/create_topic_from_inside_the_container.py
      - ./usrlib/github_data_near_real_time:/github_data_near_real_time
    
    command: ['python', 'near-real-time-producer.py', '/usrlib/helpers/getting-started-in-docker.ini']
    environment:
      - PYTHONUNBUFFERED=1
    depends_on:
      - kafka
    networks:
      - data-network

# Flink taskmanager 2
  taskmanager-2:
    image: pyflink:1.18.1
    container_name: taskmanager-2
    volumes:
      # Connectors in opt/flink/opt
      - ./connectors/flink-sql-connector-kafka-3.0.2-1.18.jar:/opt/flink/opt/flink-sql-connector-kafka-3.0.2-1.18.jar
      - ./connectors/flink-connector-cassandra_2.12-3.2.0-1.18.jar:/opt/flink/lib/flink-connector-cassandra_2.12-3.2.0-1.18.jar
      

      # Screen 1  
      - ./usrlib/near-real-time-stats-and-popularity-insights-via-flink.py:/opt/flink/usrlib/near-real-time-stats-and-popularity-insights-via-flink.py
      - ./usrlib/create_raw_events_per_sec_datastream.py:/opt/flink/usrlib/create_raw_events_per_sec_datastream.py
      - ./usrlib/near-real-time-stars-forks-via-flink.py:/opt/flink/usrlib/near-real-time-stars-forks-via-flink.py
      
      # User's pyflink jobs
      - ./usrlib/screen_2_q6_q8_flink_job.py:/opt/flink/usrlib/screen_2_q6_q8_flink_job.py
      - ./usrlib/screen_3_q9_q10_flink_job.py:/opt/flink/usrlib/screen_3_q9_q10_flink_job.py
      - ./usrlib/screen_4_q11_q15_flink_job.py:/opt/flink/usrlib/screen_4_q11_q15_flink_job.py
      
      # Screen 2 screen accelerated
      - ./usrlib/screen_2_q6_q8_flink_job_testing.py:/opt/flink/usrlib/screen_2_q6_q8_flink_job_testing.py
      - ./usrlib/screen_2_q6_q8_flink_job_testing_concurrent.py:/opt/flink/usrlib/screen_2_q6_q8_flink_job_testing_concurrent.py


      # Config file
      - ./usrlib/getting-started-in-docker.ini:/opt/flink/usrlib/getting-started-in-docker.ini
    depends_on:
      - jobmanager
    command: taskmanager
    # scale: 3
    environment:
      - | 
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        parallelism.default: 1
        taskmanager.numberOfTaskSlots: 16

      # taskmanager.rpc.port: 36429
      # taskmanager.data.port: 36430

    # ports: 
    #   - 36429:36429
    #   - 36430:36430
    
    networks:
      - data-network
      
# Near real time stats and popularity insights consumer
  python-near-real-time-stats-and-popularity-insights-consumer:
    image: python:3.10-script-executing-image
    container_name: near-real-time-stats-and-popularity-insights-consumer
    volumes: 
      - ./usrlib/near-real-time-stats-and-popularity-insights-consumer.py:/usrlib/near-real-time-stats-and-popularity-insights-consumer.py
      # - ./usrlib/thin_data.py:/usrlib/thin_data.py
      # - ./usrlib/produce_from_last_line_of_file.py:/usrlib/produce_from_last_line_of_file.py
      # - ./usrlib/check_job_status_multiple_jobs.py:/usrlib/check_job_status_multiple_jobs.py

      - ./usrlib/getting-started-in-docker.ini:/usrlib/helpers/getting-started-in-docker.ini
      # - ./usrlib/get_parsed_gharchive_files.py:/usrlib/get_parsed_gharchive_files.py
      # - ./usrlib/create_topic_from_inside_the_container.py:/usrlib/create_topic_from_inside_the_container.py
      # - ./usrlib/github_data_near_real_time:/github_data_near_real_time
    
    command: ['python', 'near-real-time-stats-and-popularity-insights-consumer.py', '/usrlib/helpers/getting-started-in-docker.ini']
    environment:
      - PYTHONUNBUFFERED=1
    depends_on:
      - kafka
    networks:
      - data-network
      


# Delete .json.gz file (to free up space)
  python-delete-json-gz-file:
    image: python:3.10-script-executing-image
    container_name: python-delete-json-gz-file
    volumes: 
      # Script deleting files in directory 'usrlib/github_data'
      - ./usrlib/delete_json_gz_file.py:/usrlib/delete_json_gz_file.py
      # Directory containing the .json.gz file to delete
      - ./usrlib/github_data:/github_data
      
    # Write the path of file to delete (e.g. /github_data/) and run the container delete it
    command: ['python', 'delete_json_gz_file.py', '/github_data/some_json_gz_file_to_delete.json.gz']
    environment:
      - PYTHONUNBUFFERED=1
      



      
